#include "SpeechRecognitionWorker.h"
#include "SpeechRecognition.h"
#include <string>
#include <sstream>


//General Log
DEFINE_LOG_CATEGORY(SpeechRecognitionPlugin);

#define print(text) if (GEngine) GEngine->AddOnScreenDebugMessage(-1, 1.5, FColor::White,text, TRUE, FVector2D(2,2))
#pragma warning(disable: 4458)
#if PLATFORM_ANDROID
static jmethodID AndroidThunkJava_CreateSpeechRecognition;
static jmethodID AndroidThunkJava_ShutdownSpeechRecognition;
static jmethodID AndroidThunkJava_InitSpeechRecognition;
static jmethodID AndroidThunkJava_SetKeywordPhrases;
static jmethodID AndroidThunkJava_SetGrammarMode;
static jmethodID AndroidThunkJava_SetLanguageModel;
static jmethodID AndroidThunkJava_LogTxt;

ASpeechRecognitionActor* FSpeechRecognitionWorker::Manager;

template <typename T>
std::string to_string(T value)
{
	std::ostringstream os;
	os << value;
	return os.str();
}

extern "C"
{

	// Log debug text - for testing
	JNIEXPORT void Java_com_sphinxue4_SpeechRecognitionWorker_logText(JNIEnv * jni, jclass clazz, jstring phrase)
	{
		const char* phraseStr = jni->GetStringUTFChars(phrase, 0);
		FString text = FString(phraseStr);
		UE_LOG(SpeechRecognitionPlugin, Log, TEXT("%s"), *text);
	}

	// Callback Methods
	JNIEXPORT void Java_com_sphinxue4_SpeechRecognitionWorker_speechRecognitionActive(JNIEnv * jni, jclass clazz)
	{
		FSpeechRecognitionWorker::Manager->SpeechRecognitionActive_method();
	}

	JNIEXPORT void Java_com_sphinxue4_SpeechRecognitionWorker_startedSpeaking(JNIEnv * jni, jclass clazz)
	{
		FSpeechRecognitionWorker::Manager->StartedSpeaking_method();
	}

	JNIEXPORT void Java_com_sphinxue4_SpeechRecognitionWorker_stoppedSpeaking(JNIEnv * jni, jclass clazz)
	{
		FSpeechRecognitionWorker::Manager->StoppedSpeaking_method();
	}

	JNIEXPORT void Java_com_sphinxue4_SpeechRecognitionWorker_phraseSpoken(JNIEnv * jni, jclass clazz, jobjectArray phrases, jfloatArray startTime, jfloatArray endTime)
	{
		//UE_LOG(SpeechRecognitionPlugin, Log, TEXT("Start of phrase spoken"));
		int phraseCnt = jni->GetArrayLength(phrases);
		int i, sum = 0;
		jfloat* startTimeVals = (jfloat*)(jni->GetFloatArrayElements(startTime, (jboolean*)false));
		jfloat* endTimeVals = (jfloat*)(jni->GetFloatArrayElements(endTime, (jboolean*)false));
		TArray<FDetectedPhrase> phraseSet;
		for (i = 0; i < phraseCnt; i++)
		{
			jstring phraseVal = (jstring)(jni->GetObjectArrayElement(phrases, i));
			float startTimeVal = startTimeVals[i];
			float endTimeVal = endTimeVals[i];
			const char *rawString = jni->GetStringUTFChars(phraseVal, 0);
			FString str = FString(UTF8_TO_TCHAR(rawString));
			const char *sil1 = "<sil>";
			const char *sil2 = "</sil>";
			FDetectedPhrase detectedPhrase;
			detectedPhrase.phrase = str;
			detectedPhrase.startTime = startTimeVal;
			detectedPhrase.endTime = endTimeVal;
			if (strcmp(rawString, sil1) != 0 && strcmp(rawString, sil2) != 0)
			{
				phraseSet.Add(detectedPhrase);
			}			
			jni->ReleaseStringUTFChars(phraseVal, rawString);
			jni->ReleaseFloatArrayElements(startTime, startTimeVals, 0);
			jni->ReleaseFloatArrayElements(endTime, endTimeVals, 0);
			jni->DeleteLocalRef(phraseVal);
		}
		// build phrase set
		FRecognisedPhrases recognisedPhrases;
		recognisedPhrases.phrases = phraseSet;
		FSpeechRecognitionWorker::Manager->WordsSpoken_method(recognisedPhrases);
	}

}

#endif


// HELPER METHODS

// Logs a message to the SpeechRecognitionPlugin category
void FSpeechRecognitionWorker::ClientMessage(FString text) {
	UE_LOG(SpeechRecognitionPlugin, Log, TEXT("%s"), *text);
}

// Splits a string into a vector of strings, using boost
vector<string> FSpeechRecognitionWorker::Split(string s) {
	vector<string> strVec;
	boost::algorithm::split(strVec, s, boost::algorithm::is_any_of("\t "));
	return strVec;
}

// strips out characters that are non alphabetical. 
// Eg. transforms "jumping(2)" to "jumping"
string FSpeechRecognitionWorker::GetOriginalString(string s) {
	string result;
	wstring wstr(s.begin(), s.end());
	wstring wstr_stripped;
	for (int j = 0; j < s.size(); j++) {
		int charInt = s[j];
		if (charInt != 40 && charInt != 41 
			&& (charInt < 48 || charInt > 57)) {
			wstr_stripped += s[j];
		}
	}
	copy(wstr_stripped.begin(), wstr_stripped.end(), back_inserter(result));
	return result;
}

// Constructor
FSpeechRecognitionWorker::FSpeechRecognitionWorker() {}

// Destructor
FSpeechRecognitionWorker::~FSpeechRecognitionWorker() {
	delete Thread;
	Thread = NULL;
}

// Call this to shutdown the speech recognition thread
void FSpeechRecognitionWorker::ShutDown() {
	Stop();
	Thread->WaitForCompletion();
	delete Thread;
}


bool FSpeechRecognitionWorker::EnableGrammarMode(FString grammarName)
{
	#if PLATFORM_WINDOWS
		const char* name = TCHAR_TO_ANSI(*grammarName);
		std::string grammarFile = contentPath_str + "model/" + langStr + "/grammars/" + name + ".gram";

		// Init config and set default params
		InitConfig();	
		cmd_ln_set_str_r(config, "-jsgf", grammarFile.c_str());

		initRequired = true;
		detectionMode = ESpeechRecognitionMode::VE_GRAMMAR;
	#endif

	#if PLATFORM_ANDROID
		// Set phrases
		JNIEnv* Env = FAndroidApplication::GetJavaEnv();
		if (Env != NULL) {
			jstring grammarStr = Env->NewStringUTF(TCHAR_TO_ANSI(*grammarName));
			AndroidThunkJava_SetGrammarMode = FJavaWrapper::FindMethod(Env, FJavaWrapper::GameActivityClassID, "AndroidThunkJava_SetGrammarMode", "(Ljava/lang/String;)V", false);
			if (!AndroidThunkJava_SetGrammarMode)
			{
				ClientMessage(FString("Method not found"));
				return JNI_ERR;
			}
			FJavaWrapper::CallVoidMethod(Env, FJavaWrapper::GameActivityThis, AndroidThunkJava_SetGrammarMode, grammarStr);
		}
	#endif
	return true;
}

bool FSpeechRecognitionWorker::EnableKeywordMode(TArray<FRecognitionPhrase> wordList)
{

#if PLATFORM_WINDOWS
	// Init config and set default params
	InitConfig();
	AddWords(wordList);
	initRequired = true;
	detectionMode = ESpeechRecognitionMode::VE_KEYWORD;
#endif


#if PLATFORM_ANDROID
	ClientMessage(FString("EnableKeywordMode"));

	// Init config and set params
	InitConfig();
	AddWords(wordList);

	// Set phrases
	JNIEnv* Env = FAndroidApplication::GetJavaEnv();
	if (Env != NULL) {
		jobjectArray phraseArray = (jobjectArray)Env->NewObjectArray(wordList.Num(), FJavaWrapper::JavaStringClass, NULL);
		jobjectArray tollerenceArray = (jobjectArray)Env->NewObjectArray(wordList.Num(), FJavaWrapper::JavaStringClass, NULL);
		jint wordCnt = wordList.Num();

		for (uint32 Param = 0; Param < wordList.Num(); Param++)
		{
			// phrase
			FString phrase = wordList[Param].phrase;
			jstring phraseStringVal = Env->NewStringUTF(TCHAR_TO_UTF8(*phrase));
			Env->SetObjectArrayElement(phraseArray, Param, phraseStringVal);
			Env->DeleteLocalRef(phraseStringVal);

			// tolerence
			int tolerance = wordList[Param].tolerance;
			FString toleranceStr = FString::FromInt(wordList[Param].tolerance);
			jstring toleranceStringVal = Env->NewStringUTF(TCHAR_TO_UTF8(*toleranceStr));
			Env->SetObjectArrayElement(tollerenceArray, Param, toleranceStringVal);
			Env->DeleteLocalRef(toleranceStringVal);
		}
		AndroidThunkJava_SetKeywordPhrases = FJavaWrapper::FindMethod(Env, FJavaWrapper::GameActivityClassID, "AndroidThunkJava_SetKeywordPhrases", "([Ljava/lang/String;[Ljava/lang/String;I)V", false);
			if (!AndroidThunkJava_SetKeywordPhrases)
			{
				ClientMessage(FString("Method not found"));
				return JNI_ERR;
			}
			FJavaWrapper::CallVoidMethod(Env, FJavaWrapper::GameActivityThis, AndroidThunkJava_SetKeywordPhrases, phraseArray, tollerenceArray, wordCnt);

	} else {
		ClientMessage(FString("Java environment reference, unable to be created."));	
	}
#endif
return true;
}

bool FSpeechRecognitionWorker::EnableLanguageModel(FString flanguageModel)
{
	#if PLATFORM_WINDOWS
		const char* name = TCHAR_TO_ANSI(*flanguageModel);
		std::string langModel = contentPath_str + "model/" + langStr + "/language_models/" + name + ".lm";

		// Init config and set default params
		InitConfig();
		ps_default_search_args(config);
		cmd_ln_set_str_r(config, "-lm", langModel.c_str());

		initRequired = true;
		detectionMode = ESpeechRecognitionMode::VE_LANGUAGE_MODEL;
	#endif

	#if PLATFORM_ANDROID

		// Set phrases
		JNIEnv* Env = FAndroidApplication::GetJavaEnv();
		if (Env != NULL) {
			jstring langModel = Env->NewStringUTF(TCHAR_TO_ANSI(*flanguageModel));
			AndroidThunkJava_SetLanguageModel = FJavaWrapper::FindMethod(Env, FJavaWrapper::GameActivityClassID, "AndroidThunkJava_SetLanguageModel", "(Ljava/lang/String;)V", false);
			if (!AndroidThunkJava_SetLanguageModel)
			{
				ClientMessage(FString("Method not found"));
				return JNI_ERR;
			}
			FJavaWrapper::CallVoidMethod(Env, FJavaWrapper::GameActivityThis, AndroidThunkJava_SetLanguageModel, langModel);
		}

	#endif

	return true;
}

void FSpeechRecognitionWorker::AddWords(TArray<FRecognitionPhrase> fkeywords) {
	this->keywords.clear();
	for (auto It = fkeywords.CreateConstIterator(); It; ++It)
	{
		FRecognitionPhrase word = *It;
		std::string wordStr = std::string(TCHAR_TO_UTF8(*word.phrase));
		transform(wordStr.begin(), wordStr.end(), wordStr.begin(), ::tolower);

		std::string tolerenceStr = "1e-";
		std::string tolS = to_string(word.tolerance);

		tolerenceStr.append(tolS);
		tolerenceStr.append("/");
		char *tolerance = new char[tolerenceStr.size() + 1];
		strcpy(tolerance, tolerenceStr.c_str());

		//tolerance = (char*) ;
		pair<map<string, string>::iterator, bool> ret;
		ret = this->keywords.insert(pair<string, string>(wordStr, tolerenceStr));
		if (ret.second == false) {
			this->keywords.erase(wordStr);
			this->keywords.insert(pair<string, string>(wordStr, tolerenceStr));
		}
	}
}

int16 FSpeechRecognitionWorker::GetCurrentVolume() {
	return currentVolume;
}

/* TODO: Saves a WAV file from the audio buffer, between the start and the end times
void FSpeechRecognitionWorker::saveWAV(FString fileName, float startTime, endTime)
{
	string wav_file = TCHAR_TO_UTF8(*fileName);
	SoundHeader header;
	header.setChannels(1);
	header.setOutputType(FORMAT_WAV_LINEAR_16);
	header.setSrate(16000);
	SoundFileWrite soundfile(wav_file.c_str(), header);
	uint16 data;

	for (int i = startByte / 2; i < endByte / 2; i++) {
		data = (audioData[i * 2 + 1] << 8) | audioData[i * 2];
		soundfile.writeSample16Bit(data);
	}	

	for (int i = startByte; i < endByte; i++) {
		data = audioData[i];
		soundfile.writeSample16Bit(data);
	}
	soundfile.close();
	fileCtr = fileCtr + 1;
}
*/

vector<int16> FSpeechRecognitionWorker::GetAudioBytes(float startTime, float endTime)
{
	vector<int16> wavData;
	startTime = startTime * 1000.0f;
	endTime = endTime * 1000.0f;
	for (const auto &myPair : rawAudio) {
		int msKey = myPair.first;
		if (msKey > startTime && msKey < endTime) {
			vector<int16> audioData = myPair.second;
			wavData.insert(wavData.end(), audioData.begin(), audioData.end());
		}
	}
	return wavData;
}


int16 FSpeechRecognitionWorker::GetAverageVolume(float startTime, float endTime) {
	int16 fcurrentVolume = 0;
	int32 peakSum = 0;

	vector<int16> peaks;
	int currentPeak = 0;

	// find the start and end frame approximations
	int startFrameKey = 0;
	int endFrameKey = 0;
	
	for (const auto &myPair : rawAudio) {
		int msKey = myPair.first;		
		if (msKey > startTime && msKey < endTime) {
			vector<int16> audioData = myPair.second;
			for (int i = 0; i < audioData.size(); i++)
			{
				if (audioData[i] > currentPeak && audioData[i] > 0)
				{
					currentPeak = audioData[i];
				}
				if (audioData[i] < 0 && currentPeak != 0)
				{
					// add peak to the collection, and reset current peak
					peaks.push_back(currentPeak);
					currentPeak = 0;
				}
			}
		}
	}

	std::string MyStdString(TCHAR_TO_UTF8(*FString::FromInt(fileCtr)));
	string wav_file = std::string(TCHAR_TO_UTF8(*FPaths::ConvertRelativePathToFull(FPaths::ProjectContentDir()))) + "/" + "temp" + MyStdString + ".wav";
	FString wav_file_fstring(wav_file.c_str());

	for (int16 peak : peaks)
	{
		peakSum += peak;
	}

	if (peakSum > 0)
	{
		fcurrentVolume = peakSum / peaks.size();
	}

	return fcurrentVolume;
}

void FSpeechRecognitionWorker::SetLanguage(ESpeechRecognitionLanguage flanguage) {

	// set Content Path
	FString contentPath = FPaths::ConvertRelativePathToFull(FPaths::ProjectContentDir());
	contentPath_str = std::string(TCHAR_TO_UTF8(*contentPath));
	contentPath_str = contentPath_str.append("SpeechRecognition//");
	std::string languageString;
	this->language = flanguage;

	// language model and dictionary paths
	switch (flanguage) {
	case ESpeechRecognitionLanguage::VE_English:
		langStr = (char*)"en";
		break;
	case ESpeechRecognitionLanguage::VE_Chinese:
		langStr = (char*)"zn";
		break;
	case ESpeechRecognitionLanguage::VE_French:
		langStr = (char*)"fr";
		break;
	case ESpeechRecognitionLanguage::VE_Spanish:
		langStr = (char*)"es";
		break;
	case ESpeechRecognitionLanguage::VE_Russian:
		langStr = (char*)"ru";
		break;
	default:
		langStr = (char*)"en";
		break;
	}

}

void FSpeechRecognitionWorker::InitConfig() {
	
#if PLATFORM_WINDOWS

	argFilePath = contentPath_str + +"model/" + langStr + "/" + langStr + ".args";
	logPath = contentPath_str + "log/";
	modelPath = contentPath_str + "model/" + langStr + "/" + langStr;
	dictionaryPath = contentPath_str + "model/" + langStr + "/" + langStr + ".dict";

	if (ps != NULL) {
		cmd_ln_free_r(config);
	}

	ClientMessage(FString(modelPath.c_str()));
	ClientMessage(FString(dictionaryPath.c_str()));


	config = cmd_ln_init(NULL, ps_args(), 1,
		"-hmm", modelPath.c_str(),
		"-dict", dictionaryPath.c_str(),
		NULL);

	// loop over, and set sphinx params	
	for (FSpeechRecognitionParam param : sphinxParams)
	{
		if (param.type == ESpeechRecognitionParamType::VE_FLOAT)
		{
			cmd_ln_set_float_r(config, param.name, atof(param.value));
		}
		if (param.type == ESpeechRecognitionParamType::VE_BOOLEAN)
		{
			if (stricmp(param.value, "0") == 0)
			{
				cmd_ln_set_boolean_r(config, param.name, false);
			}
			if (stricmp(param.value, "1") == 0)
			{
				cmd_ln_set_boolean_r(config, param.name, true);
			}
		}
		if (param.type == ESpeechRecognitionParamType::VE_STRING)
		{
			cmd_ln_set_str_r(config, param.name, param.value);
		}
		if (param.type == ESpeechRecognitionParamType::VE_INTEGER)
		{
			cmd_ln_set_int_r(config, param.name, atoi(param.value));
		}
	}

	// load dictionary
	std::ifstream file(dictionaryPath);
	std::string currentLine;
	dictionary.clear();
	while (file.good())
	{
		std::getline(file, currentLine);
		std::string rawWord = currentLine.substr(0, currentLine.find(" "));
		std::string mappedWord = "";
		std::size_t beginBracket = rawWord.find("(");
		std::size_t endBracket = rawWord.find(")");
		if (beginBracket != std::string::npos && endBracket != std::string::npos)
		{
			mappedWord = rawWord.substr(0, beginBracket);
		}
		else {
			mappedWord = rawWord;
		}

		std::set<std::string> mappings;
		if (dictionary.find(mappedWord) != dictionary.end()) {
			mappings = dictionary.at(mappedWord);
			mappings.insert(rawWord);
			dictionary[mappedWord] = mappings;
		}
		mappings.insert(rawWord);
		dictionary.insert(make_pair(mappedWord, mappings));
	}
	
	// reset params
	sphinxParams.Empty();

#endif

// TODO: Allow params to be set dynamically in Android

}

bool FSpeechRecognitionWorker::SetConfigParam(FString param, ESpeechRecognitionParamType type, FString value)
{
	char* paramName = TCHAR_TO_UTF8(*param);
	char* paramValue = TCHAR_TO_UTF8(*value);

	// Validate the incoming string, against the data type
	if (type == ESpeechRecognitionParamType::VE_FLOAT)
	{
		double validationFloat = atof(paramValue);
		if (validationFloat == 0.0F) {
			return false;
		}
		FSpeechRecognitionParam sphinxParam(paramName, type, paramValue);
		sphinxParams.Add(sphinxParam);
		return true;
	}

	if (type == ESpeechRecognitionParamType::VE_BOOLEAN)
	{
		if (value.Equals("true", ESearchCase::IgnoreCase)) {
			paramValue = (char*)"1";
			FSpeechRecognitionParam sphinxParam(paramName, type, paramValue);
			sphinxParams.Add(sphinxParam);
			return true;
		}
		if (value.Equals("false", ESearchCase::IgnoreCase)) {
			paramValue = (char*)"0";
			FSpeechRecognitionParam sphinxParam(paramName, type, paramValue);
			sphinxParams.Add(sphinxParam);
			return true;
		}
		return false;
	}

	if (type == ESpeechRecognitionParamType::VE_STRING)
	{
		FSpeechRecognitionParam sphinxParam(paramName, type, paramValue);
		sphinxParams.Add(sphinxParam);
		return true;
	}

	if (type == ESpeechRecognitionParamType::VE_INTEGER)
	{
		if (value.IsNumeric()) {
			FSpeechRecognitionParam sphinxParam(paramName, type, paramValue);
			sphinxParams.Add(sphinxParam);
			return true;
		}
		return false;
	}
	return false;
}

void FSpeechRecognitionWorker::Stop() {

#if PLATFORM_ANDROID
	// JNI
	JNIEnv* Env = FAndroidApplication::GetJavaEnv();
	if (Env != NULL) {

		// Create speech recognition worker
		AndroidThunkJava_ShutdownSpeechRecognition = FJavaWrapper::FindMethod(Env, FJavaWrapper::GameActivityClassID, "AndroidThunkJava_ShutdownSpeechRecognition", "()V", false);
		if (!AndroidThunkJava_ShutdownSpeechRecognition)
		{
			ClientMessage(FString("Method: AndroidThunkJava_ShutdownSpeechRecognition not found"));
		}
		FJavaWrapper::CallVoidMethod(Env, FJavaWrapper::GameActivityThis, AndroidThunkJava_ShutdownSpeechRecognition);
	}
	else {
		ClientMessage(FString("Java environment reference, unable to be created."));
	}

#endif

	StopTaskCounter.Increment();
}

bool FSpeechRecognitionWorker::StartThread(ASpeechRecognitionActor* manager) {
	ClientMessage(FString("StartThread"));
	Manager = manager;
	int32 threadIdx = ISpeechRecognition::Get().GetInstanceCounter();

#if PLATFORM_ANDROID
	// JNI
	JNIEnv* Env = FAndroidApplication::GetJavaEnv();
	if (Env != NULL) {

		// Create speech recognition worker
		AndroidThunkJava_CreateSpeechRecognition = FJavaWrapper::FindMethod(Env, FJavaWrapper::GameActivityClassID, "AndroidThunkJava_CreateSpeechRecognition", "()V", false);
		if (!AndroidThunkJava_CreateSpeechRecognition)
		{
			ClientMessage(FString("Method: AndroidThunkJava_CreateSpeechRecognition not found"));
			return JNI_ERR;
		}
		FJavaWrapper::CallVoidMethod(Env, FJavaWrapper::GameActivityThis, AndroidThunkJava_CreateSpeechRecognition);
	}
	else {
		ClientMessage(FString("Java environment reference, unable to be created."));
	}

#endif

	FString threadName = FString("FSpeechRecognitionWorker:") + FString::FromInt(threadIdx);
	Thread = FRunnableThread::Create(this, *threadName, 0U, TPri_Highest);
	return true;
}


uint32 FSpeechRecognitionWorker::Run() {
	
ClientMessage(FString("Running Thread"));

#if PLATFORM_ANDROID
	while (StopTaskCounter.GetValue() == 0) {
		FPlatformProcess::Sleep(0.01f);
	}
	return 0;
#endif

#if PLATFORM_WINDOWS
	// normal stuff
	bool initComplete = false;
	bool voiceAudioReady = false;
	while (StopTaskCounter.GetValue() == 0) {

		// loop until we have initialised 
		if (initRequired) {

			ps = ps_init(config);

			if (!Manager | !ps) {
				ClientMessage(FString(TEXT("Speech Recognition Thread failed to start")));
				return 1;
			}

			ClientMessage(FString(TEXT("Speech Recognition has started")));


			if (detectionMode == ESpeechRecognitionMode::VE_KEYWORD) {

				// set key-phrases
				map<string, string>::iterator it;
				char** phrases = (char**)malloc(fkeywords.size() * sizeof(char*));
				float64* tolerances;
				tolerances = (float64*)malloc(fkeywords.size() * sizeof(float64));
				int i = 0;

				for (it = fkeywords.begin(); it != keywords.end(); ++it) {

					// check if the word is in the dictionary. If missing, omit the phrase, and log
					vector<string> splitString = Split(it->first);
					vector<string>::iterator v_It;
					std::locale loc;

					bool skip = false;
					for (v_It = splitString.begin(); v_It != splitString.end(); ++v_It) {
						string orginalStr = GetOriginalString(splitString[0]);
						if (dictionary.find(orginalStr) == dictionary.end())
						{
							skip = true;
							continue;
						}
						if (dictionary.find(*v_It) == dictionary.end()) {
							std::set<string> wordSet = dictionary.at(orginalStr);
							if (wordSet.size() > 1) {
								std::string Strtxt = "The word '" + orginalStr + "' has multiple definitions, ensure multiple phrases (for each phonetic match) is added.";
								const char* txt = Strtxt.c_str();
								FString msg = FString(UTF8_TO_TCHAR(txt));
								UE_LOG(SpeechRecognitionPlugin, Log, TEXT("WARNING: %s "), *msg);
							}
							if (wordSet.find(*v_It) == wordSet.end()) {
								skip = true;
							}
						}
					}

					if (skip)
					{
						std::string Strtxt = "The phrase '" + it->first + "' can not be added, as it contains words that are not in the dictionary.";
						const char* txt = Strtxt.c_str();
						FString msg = FString(UTF8_TO_TCHAR(txt));
						UE_LOG(SpeechRecognitionPlugin, Log, TEXT("SKIPPED PHRASE: %s "), *msg);
						continue;
					}

					//add all variations of the keyword, to ensure dictionary variations are detected
					char* str;
					str = (char *)malloc(it->first.size() + 1);
					memcpy(str, it->first.c_str(), it->first.size() + 1);
					phrases[i] = str;

					char* tolStr = (char*)it->second.c_str();
					//tolerance
					int32 toleranceInt = (int32)logmath_log(ps_get_logmath(ps), atof(tolStr)) >> SENSCR_SHIFT;
					tolerances[i] = toleranceInt;
					i++;
				}

				int phraseCnt = i;
				const char** const_copy = (const char**)phrases;
				ps_set_keyphrase(ps, "keyphrase_search", const_copy, tolerances, phraseCnt);
				ps_set_search(ps, "keyphrase_search");
			}

			// attempt to open the default recording device
			if ((ad = ad_open_dev(cmd_ln_str_r(config, "-adcdev"),
				(int)cmd_ln_float32_r(config,
					"-samprate"))) == NULL) {
				ClientMessage(FString(TEXT("Failed to open audio device")));
				return 2;
			}

			if (ad_start_rec(ad) < 0) {
				ClientMessage(FString(TEXT("Failed to start recording")));
				return 3;
			}

			if (ps_start_utt(ps) < 0) {
				ClientMessage(FString(TEXT("Failed to start utterance")));
				return 4;
			}

			initRequired = false;
			initComplete = true;
			utt_started = 0;
			
			startTimeChrono = std::chrono::system_clock::now();
			ClientMessage(FString(TEXT("4")));
		}
		else {
			if (initComplete == false) {
				FPlatformProcess::Sleep(0.01f);
				if (StopTaskCounter.GetValue() == 1) {
					//ad_close(ad);
					ps_free(ps);
					cmd_ln_free_r(config);
					ClientMessage(FString("Stopping Thread"));
					return 0;
				}
				continue;
			}
		}

		if ((k = ad_read(ad, adbuf, 2048)) < 0)
			ClientMessage(FString(TEXT("Failed to read audio")));
		ps_process_raw(ps, adbuf, k, 0, 0);
		in_speech = ps_get_in_speech(ps);

		if (k > 0)
		{
			vector<int16> audioData;
			for (int i = 0; i < k; i++)
			{
				audioData.push_back(adbuf[i]);
			}

			std::chrono::time_point<std::chrono::system_clock> endTimeChrono;
			endTimeChrono = std::chrono::system_clock::now();
			auto milliseconds = chrono::duration_cast<chrono::milliseconds>(endTimeChrono - startTimeChrono).count();
			this->rawAudio.insert(pair<int32, vector<int16>>(milliseconds, audioData));
			Manager->UpdateVoiceProcedural(audioData);

			if (!voiceAudioReady)
			{
				Manager->SpeechRecognitionActive_method();
				voiceAudioReady = true;
			}
			
		}
		
		// transition from silence to listening
		if (in_speech && !utt_started) {
			utt_started = 1;
			ClientMessage(FString(TEXT("Listening")));
			Manager->StartedSpeaking_method();
			// clear start time, and buffer
			//startTimeChrono = std::chrono::system_clock::now();
			rawAudio.clear();
		}

		// transition from listening to silence
		if (!in_speech && utt_started) {

			// obtain a count of the number of frames, and the hypothesis phrase spoken
			int frame_rate = cmd_ln_int32_r(config, "-frate");
			int32 frameCount = ps_get_n_frames(ps);
			int32 score;

			// Listening period has ended
			ps_end_utt(ps);
			// re-loop, if there is no hypothesis
			const char* hyp = ps_get_hyp(ps, &score);;
			if (hyp != NULL) {

				TArray<FString> phraseStrSet;
				TArray<FDetectedPhrase> phraseSet;
				map<float, std::string>::iterator it;
				ps_seg_t *iter = ps_seg_iter(ps);

				float startF = 0;
				float endF = 10000000;


					// Orders the detected phrases into a map.
					// Key = seconds from the start of speech recognition detection.
					// Value = the phrase that was detected.
					while (iter != NULL) {
						int32 sf, ef;
						ps_seg_frames(iter, &sf, &ef);
						std::string word(ps_seg_word(iter));
						FString phraseTxt = FString(UTF8_TO_TCHAR(word.c_str()));
						float startTime = ((float)sf / frame_rate);
						float endTime = ((float)ef / frame_rate);
						// For debug purposes. Logs the raw detected phrase + time they were detected
						iter = ps_seg_next(iter);
						FDetectedPhrase phrase = FDetectedPhrase(phraseTxt, startTime, endTime, 0);

						// calculate volume
						std::chrono::time_point<std::chrono::system_clock> endTimeChrono;
						endTimeChrono = std::chrono::system_clock::now();
						auto milliseconds = chrono::duration_cast<chrono::milliseconds>(endTimeChrono - startTimeChrono).count();

						startTime = (startTime*1000.0f);
						endTime = (endTime*1000.0f);
						
						UE_LOG(SpeechRecognitionPlugin, Log, TEXT("Word: %s Start Time: %.3f End Time %.3f "), UTF8_TO_TCHAR(word.c_str()), startTime, endTime);
						phrase.avgVolume = GetAverageVolume(startTime, endTime);

						if (detectionMode == ESpeechRecognitionMode::VE_KEYWORD)
						{
							if (!phraseStrSet.Contains(phraseTxt))
							{
								phraseSet.Add(phrase);
								phraseStrSet.Add(phraseTxt);
								ClientMessage(phraseTxt);
								UE_LOG(SpeechRecognitionPlugin, Log, TEXT("Phrases: %s "), *phraseTxt);
							}
						}

						if (detectionMode == ESpeechRecognitionMode::VE_GRAMMAR)
						{
							if (phraseTxt != "<sil>")
							{
								phraseSet.Add(phrase);
								phraseStrSet.Add(phraseTxt);
								ClientMessage(phraseTxt);
								UE_LOG(SpeechRecognitionPlugin, Log, TEXT("Phrases: %s "), *phraseTxt);
							}
						}

						if (detectionMode == ESpeechRecognitionMode::VE_LANGUAGE_MODEL)
						{
							if (phraseTxt != "s" && phraseTxt != "sil")
							{
								phraseSet.Add(phrase);
								phraseStrSet.Add(phraseTxt);
								ClientMessage(phraseTxt);
								UE_LOG(SpeechRecognitionPlugin, Log, TEXT("Phrases: %s "), *phraseTxt);
							}
						}
					}
				
					FRecognisedPhrases recognisedPhrases;
					recognisedPhrases.phrases = phraseSet;
					Manager->WordsSpoken_method(recognisedPhrases);
				}
			else {
				Manager->UnknownPhrase_method();
			}
			

			if (ps_start_utt(ps) < 0)
				ClientMessage(FString(TEXT("Failed to start")));
			utt_started = 0;
			Manager->StoppedSpeaking_method();
		}
		FPlatformProcess::Sleep(0.1f);
	}

	ad_close(ad);
	ps_free(ps);
	cmd_ln_free_r(config);
	ClientMessage(FString("Stopping Thread"));
#endif
	return 0;
}